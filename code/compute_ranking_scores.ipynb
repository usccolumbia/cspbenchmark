{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78b8c95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 1, 1, 1, 1], [0, 0, 0, 0, 0])\n",
      "\n",
      " ([4, 4, 3, 1, 1], [0, 0, 60.0, 90.0, 90.0])\n",
      "\n",
      " ([3, 2, 4, 4, 1], [60.0, 80.0, 0, 0, 100.0])\n",
      "\n",
      " ([5, 3, 2, 1, 4], [20.0, 60.0, 80.0, 100.0, 40.0])\n",
      "\n",
      " ([8, 8, 7, 1, 1, 5, 5, 3, 3, 8], [0, 0, 40.0, 95.0, 95.0, 55.0, 55.0, 75.0, 75.0, 0])\n",
      "\n",
      " ([6, 7, 8, 8, 4, 8, 1, 2, 3, 5], [50.0, 40.0, 0, 0, 70.0, 0, 100.0, 90.0, 80.0, 60.0])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "'''\n",
    "We calculate ranking scores for all algorithms based on their distance metrics.\n",
    "The ranking scores on a 0-100 scale are assigned to the algorithms using a standardized scoring method to ensure fairness\n",
    "in ranking.\n",
    "For example, if there are five algorithms for comparison, five evenly distributed scores ranging from 100 to 0 are assigned\n",
    "to the five algorithms sorted by their performance from the highest to the lowest. Specifically, the algorithm in the first\n",
    "place receives a score of 100 (reflecting the smallest distance), and the second-placed algorithm earns a score of 75, followed\n",
    "by 50 for the third place, 25 for the fourth place, and 0 for the fifth place. \n",
    "In cases where multiple algorithms produced structures with identical quality/distances, they were assigned the same rank, and \n",
    "scores were averaged according to their rankings. For example, if the first and second place algorithms tie in the quality of \n",
    "their predicted structures, their scoresare set as the average [(100 + 75)/2]. Similarly, if all five algorithms have the same\n",
    "performance, their scores are set as the average of the five scores [(100 + 75 + 50 + 25 + 0)/5].\n",
    "'''\n",
    "def calculate_scores(data):\n",
    "    distance_count = {}\n",
    "    new_data = []\n",
    "    \n",
    "    for value in data:\n",
    "        #if the value is not valid, we set it as inf. \n",
    "        if value == 'N/A' or value == 'nan' or value == 5555 or value == ' ' or value == \"None\" or value == \"9999\":\n",
    "            value = float('inf')\n",
    "        new_data.append(float(value))\n",
    "    \n",
    "    data = [d for d in new_data]\n",
    "    \n",
    "    distance_count = {}\n",
    "    \n",
    "    for value in data:\n",
    "        if value in distance_count:\n",
    "            distance_count[value] += 1\n",
    "        else:\n",
    "            distance_count[value] = 1\n",
    "    \n",
    "    if len(distance_count.keys()) != len(data):\n",
    "        sorted_distance_count = dict(sorted(distance_count.items()))\n",
    "        rank_count = {}\n",
    "        rank = 1\n",
    "        final_scores = []\n",
    "        for key, count in sorted_distance_count.items():\n",
    "            rank_count[key] = rank\n",
    "            rank += count\n",
    "        \n",
    "        num_ranks = len(rank_count)\n",
    "        rank_count = dict(sorted(rank_count.items(), key=lambda item: item[1]))\n",
    "    else:\n",
    "        unique_values = sorted(set(distance_count.keys()))\n",
    "        rank_count = {value: rank for rank, value in enumerate(unique_values, 1)}\n",
    "        \n",
    "    number_dup = [distance_count[value] for value in data]\n",
    "    rank = [rank_count[value] for value in data]\n",
    "    \n",
    "    final_scores = []\n",
    "    scores_ori = np.linspace(100, 0, len(data), endpoint=False)\n",
    "    scores_ori = scores_ori.tolist()\n",
    " \n",
    "    for i in range(len(data)):\n",
    "        if data[i] == float('inf'):\n",
    "            final_scores.append(0)\n",
    "        else:\n",
    "            sum_score = sum(scores_ori[rank[i]-1:(rank[i]+number_dup[i]-1)])\n",
    "            score = round(sum_score / number_dup[i],2)\n",
    "            final_scores.append(score)\n",
    "    \n",
    "    final_ranks = rank\n",
    "    return final_ranks, final_scores\n",
    "\n",
    "data6 = ['N/A', 'N/A', 'N/A', 'N/A', 'N/A']\n",
    "scores = calculate_scores(data6)\n",
    "print(scores)\n",
    "\n",
    "data5 = [5555, 5555, 42, 1, 1]\n",
    "scores = calculate_scores(data5)\n",
    "print(\"\\n\", scores)\n",
    "\n",
    "data3 = [3, 2, 'N/A', 'nan', 0]\n",
    "scores = calculate_scores(data3)\n",
    "print(\"\\n\", scores)\n",
    "\n",
    "data4 = [5, 3, 2, 1, 4]\n",
    "scores = calculate_scores(data4)\n",
    "print(\"\\n\", scores)\n",
    "\n",
    "data5 = [5555, 5555, 42, 1, 1, 5, 5, 2, 2, 'N/A']\n",
    "scores = calculate_scores(data5)\n",
    "print(\"\\n\", scores)\n",
    "\n",
    "data6 = [55, 354, 5555, 5555, 4,'N/A',0,2,3,5]\n",
    "scores = calculate_scores(data6)\n",
    "print(\"\\n\", scores)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (metrics)",
   "language": "python",
   "name": "metrics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
