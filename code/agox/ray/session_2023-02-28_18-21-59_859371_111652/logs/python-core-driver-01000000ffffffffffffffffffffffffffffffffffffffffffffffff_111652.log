[2023-02-28 18:22:00,997 I 111652 111652] core_worker_process.cc:107: Constructing CoreWorkerProcess. pid: 111652
[2023-02-28 18:22:01,750 I 111652 111652] grpc_server.cc:105: driver server started, listening on port 39029.
[2023-02-28 18:22:01,754 I 111652 111652] core_worker.cc:185: Initializing worker at address: 10.173.98.51:39029, worker ID 01000000ffffffffffffffffffffffffffffffffffffffffffffffff, raylet c8627294c20bb3f9cc19b5b1a8525c1eadabecdf33a29b52e4d327d4
[2023-02-28 18:22:01,755 I 111652 111768] accessor.cc:608: Received notification for node id = c8627294c20bb3f9cc19b5b1a8525c1eadabecdf33a29b52e4d327d4, IsAlive = 1
[2023-02-28 18:22:01,755 I 111652 111652] io_service_pool.cc:35: IOServicePool is running with 1 io_service.
[2023-02-28 18:22:01,755 I 111652 111768] core_worker.cc:476: Event stats:


Global stats: 9 total (4 active)
Queueing time: mean = -0.000 s, max = 44.972 us, min = -0.000 s, total = -0.000 s
Execution time:  mean = 21.743 us, total = 195.685 us
Event stats:
	PeriodicalRunner.RunFnPeriodically - 3 total (1 active, 1 running), CPU time: mean = 4.368 us, total = 13.105 us
	InternalPubSubGcsService.grpc_client.GcsSubscriberCommandBatch - 1 total (0 active), CPU time: mean = 97.067 us, total = 97.067 us
	InternalPubSubGcsService.grpc_client.GcsSubscriberPoll - 1 total (1 active), CPU time: mean = 0.000 s, total = 0.000 s
	WorkerInfoGcsService.grpc_client.AddWorkerInfo - 1 total (0 active), CPU time: mean = 29.203 us, total = 29.203 us
	UNKNOWN - 1 total (1 active), CPU time: mean = 0.000 s, total = 0.000 s
	NodeInfoGcsService.grpc_client.GetAllNodeInfo - 1 total (0 active), CPU time: mean = 56.310 us, total = 56.310 us
	CoreWorker.deadline_timer.flush_profiling_events - 1 total (1 active), CPU time: mean = 0.000 s, total = 0.000 s


[2023-02-28 18:22:01,772 I 111652 111652] direct_actor_task_submitter.cc:36: Set max pending calls to -1 for actor 07233c7eeaddcae543154ea301000000
[2023-02-28 18:22:01,773 I 111652 111652] direct_actor_task_submitter.cc:36: Set max pending calls to -1 for actor 8f9a079b256ff7d7f1f6f72c01000000
[2023-02-28 18:22:01,774 I 111652 111652] direct_actor_task_submitter.cc:36: Set max pending calls to -1 for actor 441b6527ceaadf24aaa8614701000000
[2023-02-28 18:22:01,774 I 111652 111652] direct_actor_task_submitter.cc:36: Set max pending calls to -1 for actor 6d3aa3e6c73df329f9893fdd01000000
[2023-02-28 18:22:01,774 I 111652 111652] direct_actor_task_submitter.cc:36: Set max pending calls to -1 for actor c6da1be108e823cd4ffe003c01000000
[2023-02-28 18:22:03,129 I 111652 111768] actor_manager.cc:214: received notification on actor, state: ALIVE, actor_id: 07233c7eeaddcae543154ea301000000, ip address: 10.173.98.51, port: 43589, worker_id: 30e21f34a6dc908340c574710ddd1649f2481cfceb1e4a2dcc2e528c, raylet_id: c8627294c20bb3f9cc19b5b1a8525c1eadabecdf33a29b52e4d327d4, num_restarts: 0, death context type=CONTEXT_NOT_SET
[2023-02-28 18:22:03,130 I 111652 111768] direct_actor_task_submitter.cc:229: Connecting to actor 07233c7eeaddcae543154ea301000000 at worker 30e21f34a6dc908340c574710ddd1649f2481cfceb1e4a2dcc2e528c
[2023-02-28 18:22:03,130 I 111652 111768] actor_manager.cc:214: received notification on actor, state: ALIVE, actor_id: 8f9a079b256ff7d7f1f6f72c01000000, ip address: 10.173.98.51, port: 35619, worker_id: 4a66cd924ae49c4df54712b0f6085515350e32674440602d7aa4310b, raylet_id: c8627294c20bb3f9cc19b5b1a8525c1eadabecdf33a29b52e4d327d4, num_restarts: 0, death context type=CONTEXT_NOT_SET
[2023-02-28 18:22:03,130 I 111652 111768] direct_actor_task_submitter.cc:229: Connecting to actor 8f9a079b256ff7d7f1f6f72c01000000 at worker 4a66cd924ae49c4df54712b0f6085515350e32674440602d7aa4310b
[2023-02-28 18:22:03,131 I 111652 111768] actor_manager.cc:214: received notification on actor, state: ALIVE, actor_id: 441b6527ceaadf24aaa8614701000000, ip address: 10.173.98.51, port: 46111, worker_id: dae435850ee1a1ac2c4629431001a5b1770146728de7ac7aec59bea1, raylet_id: c8627294c20bb3f9cc19b5b1a8525c1eadabecdf33a29b52e4d327d4, num_restarts: 0, death context type=CONTEXT_NOT_SET
[2023-02-28 18:22:03,131 I 111652 111768] direct_actor_task_submitter.cc:229: Connecting to actor 441b6527ceaadf24aaa8614701000000 at worker dae435850ee1a1ac2c4629431001a5b1770146728de7ac7aec59bea1
[2023-02-28 18:22:03,131 I 111652 111768] actor_manager.cc:214: received notification on actor, state: ALIVE, actor_id: 6d3aa3e6c73df329f9893fdd01000000, ip address: 10.173.98.51, port: 45131, worker_id: 580a11d206a18349d26d6837515ab75e9262e7ca7fe97dd085c1a467, raylet_id: c8627294c20bb3f9cc19b5b1a8525c1eadabecdf33a29b52e4d327d4, num_restarts: 0, death context type=CONTEXT_NOT_SET
[2023-02-28 18:22:03,131 I 111652 111768] direct_actor_task_submitter.cc:229: Connecting to actor 6d3aa3e6c73df329f9893fdd01000000 at worker 580a11d206a18349d26d6837515ab75e9262e7ca7fe97dd085c1a467
[2023-02-28 18:22:03,132 I 111652 111768] actor_manager.cc:214: received notification on actor, state: ALIVE, actor_id: c6da1be108e823cd4ffe003c01000000, ip address: 10.173.98.51, port: 35683, worker_id: 2b83b9f96d996ae39a2a0c10bc37bd54dfdef69d6855c70fcbb7abc2, raylet_id: c8627294c20bb3f9cc19b5b1a8525c1eadabecdf33a29b52e4d327d4, num_restarts: 0, death context type=CONTEXT_NOT_SET
[2023-02-28 18:22:03,132 I 111652 111768] direct_actor_task_submitter.cc:229: Connecting to actor c6da1be108e823cd4ffe003c01000000 at worker 2b83b9f96d996ae39a2a0c10bc37bd54dfdef69d6855c70fcbb7abc2
[2023-02-28 18:22:11,766 W 111652 111907] metric_exporter.cc:207: [1] Export metrics to agent failed: GrpcUnknown: RPC Error message: Method not found!; RPC Error details: . This won't affect Ray, but you can lose metrics from the cluster.
[2023-02-28 18:22:11,777 I 111652 111652] core_worker.cc:593: Disconnecting to the raylet.
[2023-02-28 18:22:11,777 I 111652 111652] raylet_client.cc:163: RayletClient::Disconnect, exit_type=INTENDED_USER_EXIT, exit_detail=Shutdown by ray.shutdown()., has creation_task_exception_pb_bytes=0
[2023-02-28 18:22:11,777 I 111652 111652] core_worker.cc:540: Shutting down a core worker.
[2023-02-28 18:22:11,777 I 111652 111652] core_worker.cc:564: Disconnecting a GCS client.
[2023-02-28 18:22:11,777 I 111652 111652] core_worker.cc:568: Waiting for joining a core worker io thread. If it hangs here, there might be deadlock or a high load in the core worker io service.
[2023-02-28 18:22:11,777 I 111652 111768] core_worker.cc:691: Core worker main io service stopped.
[2023-02-28 18:22:11,777 I 111652 111652] core_worker.cc:577: Core worker ready to be deallocated.
[2023-02-28 18:22:11,777 I 111652 111652] core_worker.cc:531: Core worker is destructed
[2023-02-28 18:22:11,788 I 111652 111652] core_worker_process.cc:144: Destructing CoreWorkerProcessImpl. pid: 111652
[2023-02-28 18:22:11,788 I 111652 111652] io_service_pool.cc:47: IOServicePool is stopped.
